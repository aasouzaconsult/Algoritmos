{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Revisao_NLP2020.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"SN560eUL6DRn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1598897847235,"user_tz":180,"elapsed":1092,"user":{"displayName":"Alex Souza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgzaU4tkuCW5-K_lLAlxnpH-uV3s7g3ex2jJNHQcA=s64","userId":"07347463412914112337"}},"outputId":"b07a87e5-f9c2-469c-db18-99a8757a460b"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BOCIzeSK12FU","colab_type":"text"},"source":["# Word embeddings in 2020. Review with code examples\n","Brief overview of current word embedding methods: from Word2vec to Transformers\n"]},{"cell_type":"markdown","metadata":{"id":"3H6y7JP2AD_G","colab_type":"text"},"source":["In this article we will study word embeddings — digital representation of words suitable for processing by machine learning algorithms.\n","Originally I created this article as a general overview and compilation of current approaches to word embedding in 2020, which our AI Labs team could use from time to time as a quick refresher. I hope that my article will be useful to a wider circle of data scientists and developers. Each word embedding method in the article has a (very) short description, links for further study, and code examples in Python. All code is packed as Google Colab Notebook. So let’s begin.\n","According to Wikipedia, Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers."]},{"cell_type":"markdown","metadata":{"id":"5ys3ApPt_4E1","colab_type":"text"},"source":["## One-hot or CountVectorizing\n","The most basic method for transforming words into vectors is to count occurrence of each word in each document. Such approach is called countvectorizing or one-hot encoding.\n","The main principle of this method is to collect a set of documents (they can be words, sentences, paragraphs or even articles) and count the occurrence of every word in each document. Strictly speaking, the columns of the resulting matrix are words and the rows are documents."]},{"cell_type":"code","metadata":{"id":"gtUAAZoI_AMV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":355},"executionInfo":{"status":"ok","timestamp":1598918931953,"user_tz":180,"elapsed":1818,"user":{"displayName":"Alex Souza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgzaU4tkuCW5-K_lLAlxnpH-uV3s7g3ex2jJNHQcA=s64","userId":"07347463412914112337"}},"outputId":"3f0e0b25-ea99-43c6-847e-99cd86142420"},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","# create CountVectorizer object\n","vectorizer = CountVectorizer()\n","corpus = [\n","          'Text of the very first new sentence with the first words in sentence.',\n","          'Text of the second sentence.',\n","          'Number three with lot of words words words.',\n","          'Short text, less words.',\n","]\n","# learn the vocabulary and store CountVectorizer sparse matrix in term_frequencies\n","term_frequencies = vectorizer.fit_transform(corpus) \n","vocab = vectorizer.get_feature_names()\n","# convert sparse matrix to numpy array\n","term_frequencies = term_frequencies.toarray()\n","# visualize term frequencies \n","import seaborn as sns\n","sns.heatmap(term_frequencies, annot=True, cbar = False, xticklabels = vocab);"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"C2a7E_GpAPqt","colab_type":"text"},"source":["Another approach in countvectorizing is just to place 1 if the word is found in the document (no matter how often) and 0 if the word is not found in the document. In this case we get real ‘one-hot’ encoding."]},{"cell_type":"code","metadata":{"id":"4ZyLDK50AQU0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":318},"executionInfo":{"status":"ok","timestamp":1598919087161,"user_tz":180,"elapsed":957,"user":{"displayName":"Alex Souza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgzaU4tkuCW5-K_lLAlxnpH-uV3s7g3ex2jJNHQcA=s64","userId":"07347463412914112337"}},"outputId":"dc04795a-3ab2-4972-b1a4-d797aae56988"},"source":["one_hot_vectorizer = CountVectorizer(binary=True)\n","one_hot = one_hot_vectorizer.fit_transform(corpus).toarray()\n","sns.heatmap(one_hot, annot=True, cbar = False, xticklabels = vocab)"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7f7e6c1cb518>"]},"metadata":{"tags":[]},"execution_count":2},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"qPqfFtcyAb_k","colab_type":"text"},"source":["## TF-IDF encoding\n","With a large corpus of documents some words like ‘a’, ‘the’, ‘is’, etc. occur very frequently but they don’t carry a lot of information. Using one-hot encoding approach we can decide that these words are important because they appear in many documents. One of the ways to solve this problem is stopwords filtering, but this solution is discrete and not flexible.\n","TF-IDF (term frequency — inverse document frequency) can deal with this problem better. TF-IDF lowers the weight of commonly used words and raises the weight of rare words that occur only in current document. TF-IDF formula looks like this:\n","\n","![texto alternativo](https://miro.medium.com/max/655/1*tMTbOwNzKx06chETJkZ6ww.png)\n","\n","Where TF is calculated by dividing number of times the word occurs in the document by the total number of words in the document\n","\n","![texto alternativo](https://miro.medium.com/max/349/1*y2mxpp2Vl6W7_ZmdUISkXA.png)\n","\n","IDF (inverse document frequency), interpreted like inversed number of documents, in which the term we’re interested in occurs. N — number of documents, n(t) — number of documents with current word or term t.\n","\n","![texto alternativo](https://miro.medium.com/max/223/1*6PP8jjIoi7Oe3O1Wo4a2vQ.png)"]},{"cell_type":"code","metadata":{"id":"YeDXcmYmA1F6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":282},"executionInfo":{"status":"ok","timestamp":1598919245610,"user_tz":180,"elapsed":955,"user":{"displayName":"Alex Souza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgzaU4tkuCW5-K_lLAlxnpH-uV3s7g3ex2jJNHQcA=s64","userId":"07347463412914112337"}},"outputId":"d23a4d79-8877-4f86-e4a9-f8149baed0c9"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","import seaborn as sns\n","corpus = [\n","          'Time flies like an arrow.',\n","          'Fruit flies like a banana.'\n","]\n","vocab = ['an', 'arrow', 'banana', 'flies', 'fruit', 'like', 'time']\n","tfidf_vectorizer = TfidfVectorizer()\n","tfidf = tfidf_vectorizer.fit_transform(corpus).toarray()\n","sns.heatmap(tfidf, annot=True, cbar = False, xticklabels = vocab)"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7f7e6c0dab00>"]},"metadata":{"tags":[]},"execution_count":3},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"CDAT-TVTA-Ug","colab_type":"text"},"source":["## Word2Vec and GloVe\n","The most commonly used models for word embeddings are [word2vec](https://github.com/dav/word2vec/) and [GloVe](https://nlp.stanford.edu/projects/glove/) which are both unsupervised approaches based on the distributional hypothesis (words that occur in the same contexts tend to have similar meanings).\n","Word2Vec word embeddings are vector representations of words, that are typically learnt by an unsupervised model when fed with large amounts of text as input (e.g. Wikipedia, science, news, articles etc.). These representation of words capture semantic similarity between words among other properties. Word2Vec word embeddings are learnt in a such way, that distance between vectors for words with close meanings (“king” and “queen” for example) are closer than distance for words with complety different meanings (“king” and “carpet” for example).\n","\n","![texto alternativo](https://miro.medium.com/max/700/1*lzjgo2KaWFRPkV3LCJDr7Q.png)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gzIa8gfTJ1Nr","colab_type":"text"},"source":["Word2Vec vectors even allow some mathematic operations on vectors. For example, in this operation we are using word2vec vectors for each word:\n","\n","`king — man + woman = queen`\n","\n"]},{"cell_type":"code","metadata":{"id":"IqQ2BTWVBEJ_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":309},"executionInfo":{"status":"ok","timestamp":1598919484687,"user_tz":180,"elapsed":179038,"user":{"displayName":"Alex Souza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgzaU4tkuCW5-K_lLAlxnpH-uV3s7g3ex2jJNHQcA=s64","userId":"07347463412914112337"}},"outputId":"7ce3f03a-abfa-4252-d661-8dfbb88130eb"},"source":["# Download Google Word2Vec embeddings https://code.google.com/archive/p/word2vec/\n","!wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n","!gunzip GoogleNews-vectors-negative300.bin\n","# Try Word2Vec with Gensim\n","import gensim\n","# Load pretrained vectors from Google\n","model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n","king = model['king']"],"execution_count":4,"outputs":[{"output_type":"stream","text":["--2020-09-01 00:15:06--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n","Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.107.118\n","Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.107.118|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1647046227 (1.5G) [application/x-gzip]\n","Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n","\n","GoogleNews-vectors- 100%[===================>]   1.53G  46.1MB/s    in 36s     \n","\n","2020-09-01 00:15:42 (43.5 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n","/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n","  if np.issubdtype(vec.dtype, np.int):\n"],"name":"stderr"},{"output_type":"stream","text":["[('queen', 0.7118192911148071), ('monarch', 0.6189674139022827), ('princess', 0.5902431011199951), ('crown_prince', 0.5499460697174072), ('prince', 0.5377321243286133), ('kings', 0.5236844420433044), ('Queen_Consort', 0.5235945582389832), ('queens', 0.518113374710083), ('sultan', 0.5098593235015869), ('monarchy', 0.5087411999702454)]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WMvgwL0wB_to","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":88},"executionInfo":{"status":"ok","timestamp":1598919543320,"user_tz":180,"elapsed":961,"user":{"displayName":"Alex Souza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgzaU4tkuCW5-K_lLAlxnpH-uV3s7g3ex2jJNHQcA=s64","userId":"07347463412914112337"}},"outputId":"6dfe5cfe-feac-44c6-9291-6f95ea5a4be3"},"source":["# king - man + woman = queen\n","print(model.most_similar(positive=['woman', 'king'], negative=['man']))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n","  if np.issubdtype(vec.dtype, np.int):\n"],"name":"stderr"},{"output_type":"stream","text":["[('queen', 0.7118192911148071), ('monarch', 0.6189674139022827), ('princess', 0.5902431011199951), ('crown_prince', 0.5499460697174072), ('prince', 0.5377321243286133), ('kings', 0.5236844420433044), ('Queen_Consort', 0.5235945582389832), ('queens', 0.518113374710083), ('sultan', 0.5098593235015869), ('monarchy', 0.5087411999702454)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"75nmY7s1KC3h","colab_type":"text"},"source":["Check how similar are vectors for words ‘woman’ and ‘man’."]},{"cell_type":"code","metadata":{"id":"CyKhDcGjB4-i","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":88},"executionInfo":{"status":"ok","timestamp":1598919515120,"user_tz":180,"elapsed":964,"user":{"displayName":"Alex Souza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgzaU4tkuCW5-K_lLAlxnpH-uV3s7g3ex2jJNHQcA=s64","userId":"07347463412914112337"}},"outputId":"56efbc60-0bc3-40bf-d2b0-18d73e81a428"},"source":["print(model.similarity('woman', 'man'))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["0.76640123\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n","  if np.issubdtype(vec.dtype, np.int):\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"J6k53uxcKFy8","colab_type":"text"},"source":["Check how similar are vectors for words ‘king’ and ‘woman’."]},{"cell_type":"code","metadata":{"id":"CL5R64eMCEPg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":88},"executionInfo":{"status":"ok","timestamp":1598919561078,"user_tz":180,"elapsed":923,"user":{"displayName":"Alex Souza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgzaU4tkuCW5-K_lLAlxnpH-uV3s7g3ex2jJNHQcA=s64","userId":"07347463412914112337"}},"outputId":"9ab429fe-c2f2-4bc4-ecd7-916286897134"},"source":["print(model.similarity('king', 'woman'))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["0.12847973\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n","  if np.issubdtype(vec.dtype, np.int):\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"XUub6AZECLln","colab_type":"text"},"source":[" ### Try Glove word embeddings with Spacy\n","\n"," Another word embedding method is **Glove** (“Global Vectors”). It is based on matrix factorization techniques on the word-context matrix. It first constructs a large matrix of (words x context) co-occurrence information, i.e. for each “word” (the rows), you count how frequently we see this word in some “context” (the columns) in a large corpus. Then this matrix is factorized to a lower-dimensional (word x features) matrix, where each row now stores a vector representation for each word. In general, this is done by minimizing a “reconstruction loss”. This loss tries to find the lower-dimensional representations which can explain most of the variance in the high-dimensional data."]},{"cell_type":"code","metadata":{"id":"9JagnOyoCJHH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":581},"executionInfo":{"status":"ok","timestamp":1598919800893,"user_tz":180,"elapsed":202639,"user":{"displayName":"Alex Souza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgzaU4tkuCW5-K_lLAlxnpH-uV3s7g3ex2jJNHQcA=s64","userId":"07347463412914112337"}},"outputId":"9b376b50-b00e-47f2-91f0-b6c5d2915019"},"source":["!python3 -m spacy download en_core_web_lg"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Collecting en_core_web_lg==2.2.5\n","\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9MB)\n","\u001b[K     |████████████████████████████████| 827.9MB 1.1MB/s \n","\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.2)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.1)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (49.6.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.7.1)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.18.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2020.6.20)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.7.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.1.0)\n","Building wheels for collected packages: en-core-web-lg\n","  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-cp36-none-any.whl size=829180944 sha256=19052319a9ae4366b5032c3feb24c396a34e64813b693a427fa1c52d8f3c9f7a\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-3jsnhx9p/wheels/2a/c1/a6/fc7a877b1efca9bc6a089d6f506f16d3868408f9ff89f8dbfc\n","Successfully built en-core-web-lg\n","Installing collected packages: en-core-web-lg\n","Successfully installed en-core-web-lg-2.2.5\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_lg')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gunbij1ZChgk","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598919836921,"user_tz":180,"elapsed":32036,"user":{"displayName":"Alex Souza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgzaU4tkuCW5-K_lLAlxnpH-uV3s7g3ex2jJNHQcA=s64","userId":"07347463412914112337"}}},"source":["import spacy\n","# Load the spacy model that you have installed\n","import en_core_web_lg\n","nlp = en_core_web_lg.load()\n","# process a sentence using the model\n","doc = nlp(\"man king stands on the carpet and sees woman queen\")"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"9ew2CmiqDUaP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598919900144,"user_tz":180,"elapsed":897,"user":{"displayName":"Alex Souza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgzaU4tkuCW5-K_lLAlxnpH-uV3s7g3ex2jJNHQcA=s64","userId":"07347463412914112337"}},"outputId":"bf8efab8-b84b-4623-b86d-73fd7d70ef53"},"source":["#Find similarity between King and Queen (higher value is better).\n","doc[1].similarity(doc[9])"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.72526103"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"Tvi-BU-qDa4A","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598919918610,"user_tz":180,"elapsed":907,"user":{"displayName":"Alex Souza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgzaU4tkuCW5-K_lLAlxnpH-uV3s7g3ex2jJNHQcA=s64","userId":"07347463412914112337"}},"outputId":"1763ec33-dbea-4c1e-c9ce-be434ad1b34e"},"source":["#Find similarity between King and carpet.\n","doc[1].similarity(doc[5])"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.20431946"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"LDpuVIYjDjeP","colab_type":"text"},"source":["Check if king — man + woman = queen. We will multiply vectors for ‘man’ and ‘woman’ by two, because subtracting one vector for ‘man’ and adding the vector for ‘woman’ will do little to the original vector for “king”, likely because those “man” and “woman” are related themselves."]},{"cell_type":"code","metadata":{"id":"WOe_AgDBDfk8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598919955097,"user_tz":180,"elapsed":1025,"user":{"displayName":"Alex Souza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgzaU4tkuCW5-K_lLAlxnpH-uV3s7g3ex2jJNHQcA=s64","userId":"07347463412914112337"}},"outputId":"f57c5a77-64c2-46e6-9224-3c6e2a29c34a"},"source":["v =  doc[1].vector - (doc[0].vector*2) + (doc[8].vector*2)\n","from scipy.spatial import distance\n","import numpy as np\n","# Format the vocabulary for use in the distance function\n","vectors = [token.vector for token in doc]\n","vectors = np.array(vectors)\n","# Find the closest word below \n","closest_index = distance.cdist(np.expand_dims(v, axis = 0), vectors, metric = 'cosine').argmin()\n","output_word = doc[closest_index].text\n","print(output_word)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["queen\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JIob2qyWDoTM","colab_type":"text"},"source":["## FastText\n","[FastText](https://github.com/facebookresearch/fastText) is an extension of word2vec. FastText was developed by the team of Tomas Mikolov who created the word2vec framework in 2013.\n","\n","The main improvement of FastText over the original word2vec vectors is the inclusion of character n-grams, which allows computing word representations for words that did not appear in the training data (“out-of-vocabulary” words)."]},{"cell_type":"code","metadata":{"id":"Ebc1i5PJDx8l","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":309},"executionInfo":{"status":"ok","timestamp":1598920130008,"user_tz":180,"elapsed":42338,"user":{"displayName":"Alex Souza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgzaU4tkuCW5-K_lLAlxnpH-uV3s7g3ex2jJNHQcA=s64","userId":"07347463412914112337"}},"outputId":"2eefcdea-9a50-40ec-f359-69e34b296571"},"source":["!pip install Cython --install-option=\"--no-cython-compile\"\n","!pip install fasttext"],"execution_count":14,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pip/_internal/commands/install.py:283: UserWarning: Disabling all use of wheels due to the use of --build-options / --global-options / --install-options.\n","  cmdoptions.check_install_build_global(options)\n","Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (0.29.21)\n","Collecting fasttext\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/85/e2b368ab6d3528827b147fdb814f8189acc981a4bc2f99ab894650e05c40/fasttext-0.9.2.tar.gz (68kB)\n","\u001b[K     |████████████████████████████████| 71kB 3.2MB/s \n","\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.5.0)\n","Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (49.6.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.18.5)\n","Building wheels for collected packages: fasttext\n","  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fasttext: filename=fasttext-0.9.2-cp36-cp36m-linux_x86_64.whl size=3015284 sha256=4de7339f086a12018abbfe3f7d4b804840aeb9af6f86afc16a0b9b6967744137\n","  Stored in directory: /root/.cache/pip/wheels/98/ba/7f/b154944a1cf5a8cee91c154b75231136cc3a3321ab0e30f592\n","Successfully built fasttext\n","Installing collected packages: fasttext\n","Successfully installed fasttext-0.9.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1pEWJ8ELDykx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"13f013ce-596a-41e6-c0ef-4e0cf9281f21"},"source":["# download pre-trained language word vectors from one of 157 languges  https://fasttext.cc/docs/en/crawl-vectors.html\n","# it will take some time, about 5 minutes\n","import fasttext\n","import fasttext.util\n","fasttext.util.download_model('en', if_exists='ignore')  # English\n","ft = fasttext.load_model('cc.en.300.bin')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tRZfVvY2F-ko","colab_type":"code","colab":{}},"source":["#Create an embedding for the word ‘king’.\n","ft.get_word_vector('king')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JjPqCBo3GCei","colab_type":"code","colab":{}},"source":["# Get most similar words for the word ‘king’.\n","ft.get_nearest_neighbors('king')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f6Bm-pzAGHop","colab_type":"code","colab":{}},"source":["#Test model ability to create vectors for unknown words.\n","'king-warrior' in ft.words"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0HBs_9G0GPoN","colab_type":"code","colab":{}},"source":["# Get most similar words for unknown word ‘king-warrior’.\n","ft.get_nearest_neighbors('king-warrior')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tWd4MKEdGWYP","colab_type":"text"},"source":["## ELMo (Embeddings from Language Models)\n","Unlike traditional word embeddings such as word2vec and GLoVe, the ELMo vector assigned to a token or a word depends on current context and is actually a function of the entire sentence containing that word. So the same word can have different word vectors under different contexts. Also ELMo representations are purely character based so they are not limited to any predefined vocabulary.\n","\n","Description from the official site:\n","\n","ELMo is a deep contextualized word representation that models both (1) complex characteristics of the word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). These word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. They can be easily added to existing models and significantly improve the state of the art across a broad range of challenging NLP problems, including question answering, textual entailment and sentiment analysis."]},{"cell_type":"code","metadata":{"id":"crkn6osyGitG","colab_type":"code","colab":{}},"source":["# use tensorflow 1.x for ELMo, because trere are still no ELMo for tensorflow 2.0\n","%tensorflow_version 1.x\n","import tensorflow_hub as hub\n","import tensorflow as tf\n","# Download pretrained ELMo model from Tensorflow Hub https://tfhub.dev/google/elmo/3\n","elmo = hub.Module(\"https://tfhub.dev/google/elmo/3\", trainable=True)\n","sentences =  \\\n","['king arthur, also called arthur or aathur pendragon, legendary british king who appears in a cycle of \\\n","medieval romances (known as the matter of britain) as the sovereign of a knightly fellowship of the round table.', \n","'it is not certain how these legends originated or whether the figure of arthur was based on a historical person.', \n","'the legend possibly originated either in wales or in those parts of northern britain inhabited by brythonic-speaking celts.', \n","'for a fuller treatment of the stories about king arthur, see also arthurian legend.']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jyCDdi-wGnDX","colab_type":"text"},"source":["In order to send sentences to the model we need to split them into the arrays of words and pad arrays to the same length. Also we will create ‘mask’ array that will show whether element is a real word or a padding symbol (in our case — ‘_’). We will use ‘mask’ array for visualization later to show only real words."]},{"cell_type":"code","metadata":{"id":"NGwqqTuxGpCg","colab_type":"code","colab":{}},"source":["words = []\n","mask = []\n","masked_words = []\n","\n","for sent in sentences:\n","  splitted = sent.split()\n","  for i in range(36):\n","    try:\n","      words.append(splitted[i])\n","    except:\n","      words.append('_')\n","for word in words:\n","  if word == \"_\":\n","    mask.append(False)\n","  else:\n","    mask.append(True)\n","    masked_words.append(word)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"99UGnfNFGswS","colab_type":"code","colab":{}},"source":["#Create embeddings with ELMo:\n","embeddings = elmo(\n","    sentences,\n","    signature=\"default\",\n","    as_dict=True)[\"elmo\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XPuCAmymGynB","colab_type":"code","colab":{}},"source":["##Convert Tensorflow tensors to numpy array.\n","%%time\n","with tf.Session() as sess:\n","  sess.run(tf.global_variables_initializer())\n","  sess.run(tf.tables_initializer())\n","  x = sess.run(embeddings)\n","embs = x.reshape(-1, 1024)\n","masked_embs = embs[mask]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z2Ol4ByHG2dx","colab_type":"code","colab":{}},"source":["# Visualize embeddings using PCA.\n","from sklearn.decomposition import PCA\n","pca = PCA(n_components=10)\n","y = pca.fit_transform(masked_embs)\n","from sklearn.manifold import TSNE\n","y = TSNE(n_components=2).fit_transform(y)\n","import plotly as py\n","import plotly.graph_objs as go\n","data = [\n","    go.Scatter(\n","        x=[i[0] for i in y],\n","        y=[i[1] for i in y],\n","        mode='markers',\n","        text=[i for i in masked_words],\n","    marker=dict(\n","        size=16,\n","        color = [len(i) for i in masked_words], #set color equal to a variable\n","        opacity= 0.8,\n","        colorscale='Viridis',\n","        showscale=False\n","    )\n","    )\n","]\n","layout = go.Layout()\n","layout = dict(\n","              yaxis = dict(zeroline = False),\n","              xaxis = dict(zeroline = False)\n","             )\n","fig = go.Figure(data=data, layout=layout)\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"urgrfLl1HVFO","colab_type":"text"},"source":["## Transformers\n","At last it’s time for current state-of-the-art approach — Transformers. Famous GPT-2, BERT, CTRL are all Transformers-based and produce context-sensitive embeddings like ELMo. But unlike ELMo Transformers do not use RNN, they do not require to process words in sentence sequentially one-by-one. All words in the sentence are processed in parallel, this approach speeds up processing and solves vanishing gradient problem.\n","Transformers use the attention mechanizm to describe the connections and dependencies of each specific word with all other words in the sentence. This mechanism and the main principles of Transformers described in detail in a beautifully illustrated article by Jay Alammar.\n","\n","![texto alternativo](https://miro.medium.com/max/700/0*lnxCxwWsz_uUnunF.png)"]},{"cell_type":"markdown","metadata":{"id":"b4U3j1m6H4R6","colab_type":"text"},"source":["For our example we will use brilliant Transformers library which contains the latest Transformers-based models (such as BERT, XLNet, DialoGPT or GPT-2).\n","Let’s make some embeddings with BERT. Firstly we will need to install Transformers library."]},{"cell_type":"code","metadata":{"id":"opySAgLOH4z6","colab_type":"code","colab":{}},"source":["!pip install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k-_-mwzCH7Ko","colab_type":"text"},"source":["Now we import pytorch, the pretrained BERT model, and a BERT tokenizer that will do all the needed work of converting sentences into format appropriate for BERT (tokenizing itself and adding special tokens like [SEP] and [CLS])."]},{"cell_type":"code","metadata":{"id":"fH56JkTOH-3z","colab_type":"code","colab":{}},"source":["import torch\n","torch.manual_seed(0)\n","from transformers import BertTokenizer, BertModel\n","import logging\n","import matplotlib.pyplot as plt\n","% matplotlib inline\n","# Load pre-trained model tokenizer (vocabulary)\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lSefrshsICPw","colab_type":"text"},"source":["Enter some sentences and tokenize them."]},{"cell_type":"code","metadata":{"id":"Khq0nCLaIFLU","colab_type":"code","colab":{}},"source":["sentences =  \\\n","['king arthur, also called arthur or aathur pendragon, legendary british king who appears in a cycle of \\\n","medieval romances (known as the matter of britain) as the sovereign of a knightly fellowship of the round table.', \n","'it is not certain how these legends originated or whether the figure of arthur was based on a historical person.', \n","'the legend possibly originated either in wales or in those parts of northern britain inhabited by brythonic-speaking celts.', \n","'for a fuller treatment of the stories about king arthur, see also arthurian legend.']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m14rj68KIHNz","colab_type":"code","colab":{}},"source":["# Print the original sentence.\n","print(' Original: ', sentences[0][:99])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bGvqq4qgIJ7W","colab_type":"code","colab":{}},"source":["# Print the sentence splitted into tokens.\n","print('Tokenized: ', tokenizer.tokenize(sentences[0])[:15])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TAYT-r8WIMP7","colab_type":"code","colab":{}},"source":["# Print the sentence mapped to token ids.\n","print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0]))[:15])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L37uLQRgIVU5","colab_type":"text"},"source":["Note that some tokens may look like this: [‘aa’, ‘##th’, ‘##ur’, ‘pen’, ‘##dra’, ‘##gon’]. This is because of the BERT tokenizer was created with a WordPiece model. This model greedily creates a fixed-size vocabulary of individual characters, subwords, and words that best fits our language data. BERT tokenizer uses vocabulary that contains all English characters plus the ~30,000 most common words and subwords found in the English language corpus the model is trained on. So, if the word is not mentioned in a vocabulary, that words is splitted into subwords and characters. The two hash signs (##) before some subwords shows that subword is part of a larger word and preceded by another subword.\n","\n","We will use tokenizer.encode_plus function, that will:\n","\n","- Split the sentence into tokens.\n","- Add the special [CLS] and [SEP] tokens.\n","- Map the tokens to their IDs.\n","- Pad or truncate all sentences to the same length."]},{"cell_type":"code","metadata":{"id":"RFqgbuFcIfT6","colab_type":"code","colab":{}},"source":["# Tokenize all of the sentences and map tokens to word IDs.\n","input_ids = []\n","attention_masks = []\n","tokenized_texts = []\n","for sent in sentences:\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      \n","                        add_special_tokens = True,\n","                        truncation=True,\n","                        max_length = 48,          \n","                        pad_to_max_length = True,                        \n","                        return_tensors = 'pt',    \n","                   )\n","    # Save tokens from sentence as a separate array. \n","    marked_text = \"[CLS] \" + sent + \" [SEP]\"\n","    tokenized_texts.append(tokenizer.tokenize(marked_text))\n","    \n","    # Add the encoded sentence to the list.    \n","    input_ids.append(encoded_dict['input_ids'])\n","# Convert the list into tensor.\n","input_ids = torch.cat(input_ids, dim=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y2z87FRkIiOa","colab_type":"text"},"source":["Segment ID. BERT is trained on and expects sentence pairs using 1s and 0s to distinguish between the two sentences. We will encode each sentence separately so we will just mark each token in each sentence with 1."]},{"cell_type":"code","metadata":{"id":"Iggn2SXSIjEM","colab_type":"code","colab":{}},"source":["segments_ids = torch.ones_like(input_ids)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AByP7kCAIlYC","colab_type":"text"},"source":["Now we can call BERT model and finally get model hidden states from which we will create word embeddings."]},{"cell_type":"code","metadata":{"id":"PrHVECURIkk2","colab_type":"code","colab":{}},"source":["with torch.no_grad():\n","    outputs = model(input_ids, segments_ids)\n","    hidden_states = outputs[2]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"86cAMOk5IomJ","colab_type":"text"},"source":["Let’s examine what we’ve got."]},{"cell_type":"code","metadata":{"id":"OE-pxTg_IkfI","colab_type":"code","colab":{}},"source":["print (\"Number of layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")\n","print (\"Number of batches:\", len(hidden_states[0]))\n","print (\"Number of tokens:\", len(hidden_states[0][0]))\n","print (\"Number of hidden units:\", len(hidden_states[0][0][0]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-OyO0869Iuqe","colab_type":"code","colab":{}},"source":["# Concatenate the tensors for all layers. \n","token_embeddings = torch.stack(hidden_states, dim=0)\n","# Swap dimensions, so we get tensors in format: [sentence, tokens, hidden layes, features]\n","token_embeddings = token_embeddings.permute(1,2,0,3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u5kdxDwfIxTz","colab_type":"text"},"source":["We will use last four hidden layers to create each word embedding."]},{"cell_type":"code","metadata":{"id":"0qiHrqs_IyDb","colab_type":"code","colab":{}},"source":["processed_embeddings = token_embeddings[:, :, 9:, :]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tqf9LXM3I0u1","colab_type":"text"},"source":["Concatenate four layers for each token to create embeddings"]},{"cell_type":"code","metadata":{"id":"W3V_Hsv9I3E5","colab_type":"code","colab":{}},"source":["embeddings = torch.reshape(processed_embeddings, (4, 48, -1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eXqG2U34I49C","colab_type":"text"},"source":["Let’s examine embeddings for the first sentence. Firstly we need to get ids of tokens we need to compare."]},{"cell_type":"code","metadata":{"id":"YHtXotiSI7An","colab_type":"code","colab":{}},"source":["for i, token_str in enumerate(tokenized_texts[0]):\n","  print (i, token_str)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-XhjE0zFI9is","colab_type":"text"},"source":["We can see that word ‘king’ is placed at indexes 1 and 17. We will check distance between embeddings 1 and 17. Also, we will check if embedding for the word ‘arthur’ is closer to ‘king’ then to the word ‘table’."]},{"cell_type":"code","metadata":{"id":"3I2N5DhOI-gz","colab_type":"code","colab":{}},"source":["from scipy.spatial.distance import cosine\n"," \n","kings = cosine(embeddings[0][1], embeddings[0][17])\n","king_table = cosine(embeddings[0][1], embeddings[0][46])\n","king_archtur = cosine(embeddings[0][2], embeddings[0][1])\n"," \n","print('Distance for two kings:  %.2f' % kings)\n","print('Distance from king to table:  %.2f' % king_table)\n","print('Distance from Archtur to king:  %.2f' % king_archtur)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2Sbc9weGJDSW","colab_type":"text"},"source":["So we see that embeddings for two ‘kings’ are quite similar but not the same, and Archtur is closer to be a king than a table.\n","Things may be simplier with simplerepresentations module. This module does all the work we did earlier — extracts needed hidden states from BERT and creates embeddings in a few lines of code."]},{"cell_type":"code","metadata":{"id":"5N5KesZwJGdR","colab_type":"code","colab":{}},"source":["!pip install simplerepresentations\n","import torch\n","from simplerepresentations import RepresentationModel\n","torch.manual_seed(0)\n","model_type = 'bert'\n","model_name = 'bert-base-uncased'\n","representation_model = RepresentationModel(\n","  model_type=model_type,\n","  model_name=model_name,\n","  batch_size=4,\n","  max_seq_length=48, \n","  combination_method='cat', \n","  last_hidden_to_use=4 \n"," )\n","text_a = sentences\n","all_sentences_representations, all_tokens_representations = representation_model(text_a=text_a)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wLmqGD1oJHva","colab_type":"text"},"source":["Check distaces between Archtur, king and table."]},{"cell_type":"code","metadata":{"id":"BlhZJC7XJLCH","colab_type":"code","colab":{}},"source":["from scipy.spatial.distance import cosine\n","kings = cosine(all_tokens_representations[0][1], all_tokens_representations[0][17])\n","king_table = cosine(all_tokens_representations[0][1], all_tokens_representations[0][46])\n","king_archtur = cosine(all_tokens_representations[0][2], all_tokens_representations[0][1])\n","print('Distance for two kings:  %.2f' % kings)\n","print('Distance from king to table:  %.2f' % king_table)\n","print('Distance from Archtur to king:  %.2f' % king_archtur)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jvxMkUd6JNUB","colab_type":"text"},"source":["Same results, less code."]},{"cell_type":"markdown","metadata":{"id":"Ow0VPi2UJQsh","colab_type":"text"},"source":["## Conclusion\n","I hope that after reading this article you have formed an idea of the current approaches to word embeddings and began to understand how to quickly implement these approaches in Python. The world of NLP is diverse and there are many more models and methods for embeddings. In my article I focused on the most common and those that we ourselves often use in our work. You can find additional information in the References section.\n","\n","Link original\n","https://towardsdatascience.com/word-embeddings-in-2020-review-with-code-examples-11eb39a1ee6d"]},{"cell_type":"markdown","metadata":{"id":"J2RNbTe5LmBK","colab_type":"text"},"source":["![texto alternativo](https://miro.medium.com/max/1000/1*1Kr-mgVDevo63WPYm37gBA.png)"]}]}