{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Rede Neural - exemplo.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPGJMibGsRwI3R4CyeHjLJx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Redes neurais\n","\n","Rede Neural Artificial (RNA) pode ser definida como uma estrutura complexa interligada por elementos de processamento simples (neur√¥nios), que possuem a capacidade de realizar opera√ß√µes como c√°lculos em paralelo, para processamento de dados e representa√ß√£o de conhecimento. Seu primeiro conceito foi introduzido em 1943, mas ganhou popularidade algumas d√©cadas depois com a introdu√ß√£o de algoritmos de treinamento como o backpropagation, que permite a realiza√ß√£o de um treinamento posterior para aperfei√ßoar os resultados do modelo."],"metadata":{"id":"mPVvnxTTpd3Z"}},{"cell_type":"markdown","source":["## Estudo: Conjunto de dados de sementes de trigo üìö\n","\n"],"metadata":{"id":"nbLkuizApZaN"}},{"cell_type":"markdown","source":["O conjunto de dados de sementes envolve a previs√£o de esp√©cies com medidas de sementes de diferentes variedades de trigo.\n","\n","Existem 201 registros e 7 vari√°veis ‚Äã‚Äãde entrada num√©ricas. √â um problema de classifica√ß√£o com 3 classes de sa√≠da. A escala para cada valor num√©rico de entrada varia, portanto, pode ser necess√°ria alguma normaliza√ß√£o de dados para uso com algoritmos que ponderam entradas como o algoritmo de retropropaga√ß√£o.\n","\n","Voc√™ pode aprender mais e baixar o conjunto de dados de sementes no [Reposit√≥rio de Aprendizado de M√°quina da UCI](http://archive.ics.uci.edu/ml/datasets/seeds).\n","- Fa√ßa o download do conjunto de dados de sementes e coloque-o em seu diret√≥rio de trabalho atual com o nome de arquivo **seeds_dataset.csv**.\n","- O conjunto de dados est√° no formato separado por tabula√ß√£o, portanto, voc√™ deve convert√™-lo para CSV usando um editor de texto ou um programa de planilha. Se preferir, pode fazer download do conjunto de dados no formato CSV diretamente:\n","  - [Baixar conjunto de dados de sementes de trigo](https://raw.githubusercontent.com/jbrownlee/Datasets/master/wheat-seeds.csv)"],"metadata":{"id":"_PrKGBz1p4fD"}},{"cell_type":"markdown","source":["Importando as bibliotecas"],"metadata":{"id":"anMhWtoCq0hU"}},{"cell_type":"code","source":["# Backprop on the Seeds Dataset\n","from random import seed\n","from random import randrange\n","from random import random\n","from csv import reader\n","from math import exp"],"metadata":{"id":"xlyPx3SQKNn4","executionInfo":{"status":"ok","timestamp":1654027220696,"user_tz":180,"elapsed":292,"user":{"displayName":"Alex Souza","userId":"07347463412914112337"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["### Inicialize a rede"],"metadata":{"id":"tl5KvENGq2fE"}},{"cell_type":"markdown","source":["Vamos come√ßar com algo f√°cil, a cria√ß√£o de uma nova rede pronta para treinamento.\n","\n","Cada neur√¥nio tem um conjunto de pesos que precisam ser mantidos. Um peso para cada conex√£o de entrada e um peso adicional para a polariza√ß√£o. Precisamos armazenar propriedades adicionais para um neur√¥nio durante o treinamento; portanto, usaremos um dicion√°rio para representar cada neur√¥nio e armazenar propriedades por nomes como `pesos (weights)` para os pesos.\n","\n","Uma rede √© organizada em camadas. A camada de entrada √© realmente apenas uma linha do nosso conjunto de dados de treinamento. A primeira camada real √© a camada oculta. Isso √© seguido pela camada de sa√≠da que possui um neur√¥nio para cada valor de classe.\n","\n","Organizaremos as camadas como matrizes de dicion√°rios e trataremos toda a rede como uma matriz de camadas.\n","\n","√â uma boa pr√°tica inicializar os pesos da rede para pequenos n√∫meros aleat√≥rios. Nesse caso, usaremos n√∫meros aleat√≥rios no intervalo de 0 a 1.\n","\n","Abaixo est√° uma fun√ß√£o denominada `initialize_network()` que cria uma nova rede neural pronta para treinamento. Ele aceita tr√™s par√¢metros, o n√∫mero de entradas, o n√∫mero de neur√¥nios a ter na camada oculta e o n√∫mero de sa√≠das.\n","\n","Voc√™ pode ver que, para a camada oculta, criamos `n_hidden` neur√¥nios e cada neur√¥nio na camada oculta possui `n_inputs + 1` pesos, um para cada coluna de entrada em um conjunto de dados e um adicional para o vi√©s.\n","\n","Voc√™ tamb√©m pode ver que a camada de sa√≠da que se conecta √† camada oculta possui `n_outputs` de neur√¥nios, cada um com `n_hidden + 1` pesos. Isso significa que cada neur√¥nio na camada de sa√≠da se conecta a (tem um peso para) cada neur√¥nio na camada oculta."],"metadata":{"id":"LtcZb2bRtOPP"}},{"cell_type":"code","source":["# Initialize a network\n","def initialize_network(n_inputs, n_hidden, n_outputs):\n","   network = list()\n","   hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n","   network.append(hidden_layer)\n","   output_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n","   network.append(output_layer)\n","   return network"],"metadata":{"id":"-woxLb7nsGgd","executionInfo":{"status":"ok","timestamp":1654031337984,"user_tz":180,"elapsed":5,"user":{"displayName":"Alex Souza","userId":"07347463412914112337"}}},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":["Vamos testar esta fun√ß√£o. Abaixo est√° um exemplo completo que cria uma pequena rede."],"metadata":{"id":"YH3LDRTbspd3"}},{"cell_type":"code","source":["from random import seed\n","from random import random\n","# Initialize a network\n","def initialize_network(n_inputs, n_hidden, n_outputs):\n","   network = list()\n","   hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n","   network.append(hidden_layer)\n","   output_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n","   network.append(output_layer)\n","   return network\n","   \n","seed(1)\n","network = initialize_network(2, 1, 2)\n","for layer in network:\n","   print(layer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"veYjxzgJsp3g","executionInfo":{"status":"ok","timestamp":1654027121920,"user_tz":180,"elapsed":935,"user":{"displayName":"Alex Souza","userId":"07347463412914112337"}},"outputId":"20853332-d67d-4889-9de3-9f9c00ecb243"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}]\n","[{'weights': [0.2550690257394217, 0.49543508709194095]}, {'weights': [0.4494910647887381, 0.651592972722763]}]\n"]}]},{"cell_type":"markdown","source":["Executando o exemplo, voc√™ pode ver que o c√≥digo imprime cada camada uma por uma. Voc√™ pode ver que a camada oculta possui um neur√¥nio com 2 pesos de entrada mais o vi√©s. A camada de sa√≠da possui 2 neur√¥nios, cada um com 1 peso mais o vi√©s."],"metadata":{"id":"COZcYJkus0Ph"}},{"cell_type":"markdown","source":["‚ñ∂ Agora que sabemos como criar e inicializar uma rede, vamos ver como podemos us√°-la para calcular uma sa√≠da."],"metadata":{"id":"LUsuC8eus8mj"}},{"cell_type":"markdown","source":["### Propagar para a frente (Forward Propagate)"],"metadata":{"id":"ge9U69uatE1Z"}},{"cell_type":"markdown","source":["Podemos calcular uma sa√≠da de uma rede neural propagando um sinal de entrada atrav√©s de cada camada at√© que a camada de sa√≠da produza seus valores.\n","\n","Chamamos isso de **propaga√ß√£o direta**.\n","\n","√â a t√©cnica de que precisamos para gerar previs√µes durante o treinamento que precisar√° ser corrigida e √© o m√©todo que precisaremos depois que a rede for treinada para fazer previs√µes de novos dados.\n","\n","Podemos dividir a propaga√ß√£o em tr√™s partes:\n","1.  Ativa√ß√£o do neur√¥nio.\n","2.  Transfer√™ncia de neur√¥nios\n","3.  Propaga√ß√£o para a frente."],"metadata":{"id":"mWGXUG_gtVyk"}},{"cell_type":"markdown","source":["#### Ativa√ß√£o do Neur√¥nio"],"metadata":{"id":"wxtgTv-Kt1z-"}},{"cell_type":"markdown","source":["O primeiro passo √© calcular a ativa√ß√£o de um neur√¥nio que recebe uma entrada.\n","\n","A entrada pode ser uma linha do nosso conjunto de dados de treinamento, como no caso da camada oculta. Tamb√©m podem ser as sa√≠das de cada neur√¥nio na camada oculta, no caso da camada de sa√≠da.\n","\n","A ativa√ß√£o do neur√¥nio √© calculada como a soma ponderada das entradas. Muito parecido com regress√£o linear.\n","\n","`activation = sum(weight_i * input_i) + bias`\n","\n","Onde `peso` √© um peso de rede, `entrada` √© uma entrada, `i` √© o √≠ndice de um peso ou uma entrada e `vi√©s` √© um peso especial que n√£o tem entrada para multiplicar (ou voc√™ pode pensar na entrada como sempre sendo 1.0).\n","\n","Abaixo est√° uma implementa√ß√£o disso em uma fun√ß√£o chamada `activate()`. Voc√™ pode ver que a fun√ß√£o assume que o vi√©s √© o √∫ltimo peso na lista de pesos. Isso ajuda aqui e mais tarde a facilitar a leitura do c√≥digo."],"metadata":{"id":"BA3Ljm1-t5k2"}},{"cell_type":"code","source":["# Calculate neuron activation for an input\n","def activate(weights, inputs):\n","   activation = weights[-1]\n","   for i in range(len(weights)-1):\n","      activation += weights[i] * inputs[i]\n","   return activation"],"metadata":{"id":"JcySKe9muBI3","executionInfo":{"status":"ok","timestamp":1654031207354,"user_tz":180,"elapsed":280,"user":{"displayName":"Alex Souza","userId":"07347463412914112337"}}},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":["Agora, vamos ver como usar a ativa√ß√£o do neur√¥nio."],"metadata":{"id":"2gjiSpiiubT3"}},{"cell_type":"markdown","source":["#### Transfer√™ncia de neur√¥nios"],"metadata":{"id":"ieP55uWXuhZD"}},{"cell_type":"markdown","source":["Depois que um neur√¥nio √© ativado, precisamos transferir a ativa√ß√£o para ver qual √© realmente a sa√≠da do neur√¥nio.\n","\n","Diferentes fun√ß√µes de transfer√™ncia podem ser usadas. √â tradicional usar a [**fun√ß√£o de ativa√ß√£o sigm√≥ide**](https://en.wikipedia.org/wiki/Sigmoid_function) , mas voc√™ tamb√©m pode usar a [**fun√ß√£o tanh ( tangente hiperb√≥lica )**](https://en.wikipedia.org/wiki/Hyperbolic_functions) para transferir sa√≠das. Mais recentemente, a [**fun√ß√£o de transfer√™ncia de retificadores**](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) tem sido popular em grandes redes de aprendizado profundo.\n","\n","A fun√ß√£o de ativa√ß√£o sigm√≥ide se parece com uma forma de S, tamb√©m √© chamada de fun√ß√£o log√≠stica. Ele pode pegar qualquer valor de entrada e produzir um n√∫mero entre 0 e 1 em uma curva S. Tamb√©m √© uma fun√ß√£o da qual podemos calcular facilmente a derivada (inclina√ß√£o) de que precisaremos mais tarde ao retropropagar o erro.\n","\n","Podemos transferir uma fun√ß√£o de ativa√ß√£o usando a fun√ß√£o sigm√≥ide da seguinte maneira:\n","`output = 1 / (1 + e^(-activation))`\n","\n","Onde `e` √© a base dos logaritmos naturais ([n√∫mero de Euler](https://en.wikipedia.org/wiki/E_(mathematical_constant))).\n","\n","Abaixo est√° uma fun√ß√£o chamada `transfer()` que implementa a equa√ß√£o sigm√≥ide."],"metadata":{"id":"nK87AMW1ui_7"}},{"cell_type":"code","source":["# Transfer neuron activation\n","def transfer(activation):\n","   return 1.0 / (1.0 + exp(-activation))"],"metadata":{"id":"_0bD1l_ouaAt","executionInfo":{"status":"ok","timestamp":1654031217529,"user_tz":180,"elapsed":269,"user":{"displayName":"Alex Souza","userId":"07347463412914112337"}}},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":["Agora que temos as pe√ßas, vamos ver como elas s√£o usadas."],"metadata":{"id":"VmLoH5HbvXge"}},{"cell_type":"markdown","source":["#### Propaga√ß√£o direta"],"metadata":{"id":"EKaoQ8snvZeA"}},{"cell_type":"markdown","source":["A propaga√ß√£o direta de uma entrada √© simples.\n","\n","Trabalhamos atrav√©s de cada camada da nossa rede, calculando as sa√≠das para cada neur√¥nio. Todas as sa√≠das de uma camada se tornam entradas para os neur√¥nios na pr√≥xima camada.\n","\n","Abaixo est√° uma fun√ß√£o chamada `forward_propagate()` que implementa a propaga√ß√£o direta de uma linha de dados de nosso conjunto de dados com nossa rede neural.\n","\n","Voc√™ pode ver que o valor de sa√≠da de um neur√¥nio √© armazenado no neur√¥nio com o nome `output`. Voc√™ tamb√©m pode ver que coletamos as sa√≠das para uma camada em uma matriz chamada new_inputs que se torna a `entrada (inputs)` da matriz e √© usada como entrada para a camada a seguir.\n","\n","A fun√ß√£o retorna as sa√≠das da √∫ltima camada, tamb√©m chamada camada de sa√≠da."],"metadata":{"id":"LcZDd4MvvcWI"}},{"cell_type":"code","source":["# Forward propagate input to a network output\n","def forward_propagate(network, row):\n","   inputs = row\n","   for layer in network:\n","      new_inputs = []\n","      for neuron in layer:\n","         activation = activate(neuron['weights'], inputs)\n","         neuron['output'] = transfer(activation)\n","         new_inputs.append(neuron['output'])\n","      inputs = new_inputs\n","   return inputs"],"metadata":{"id":"axq43R9tvrs-","executionInfo":{"status":"ok","timestamp":1654031232418,"user_tz":180,"elapsed":291,"user":{"displayName":"Alex Souza","userId":"07347463412914112337"}}},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":["##### Exemplo"],"metadata":{"id":"THgckyicvqnu"}},{"cell_type":"markdown","source":["Vamos juntar todas essas pe√ßas e testar a propaga√ß√£o direta da nossa rede.\n","\n","Definimos nossa rede em linha com um neur√¥nio oculto que espera 2 valores de entrada e uma camada de sa√≠da com dois neur√¥nios."],"metadata":{"id":"fhsTsL6HwB3P"}},{"cell_type":"code","source":["# test forward propagation\n","network = [[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}], [{'weights': [0.2550690257394217, 0.49543508709194095]}, {'weights': [0.4494910647887381, 0.651592972722763]}]]\n","row = [1, 0, None]\n","output = forward_propagate(network, row)\n","print(output)\n","\n","# A execu√ß√£o do exemplo propaga o padr√£o de entrada [1, 0] e produz um valor de sa√≠da que √© impresso. Como a camada de sa√≠da possui dois neur√¥nios, obtemos uma lista de dois n√∫meros como sa√≠da.\n","# Os valores reais de sa√≠da s√£o apenas absurdos por enquanto, mas a seguir, come√ßaremos a aprender como tornar os pesos nos neur√¥nios mais √∫teis."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GlsqWIY3wK1K","executionInfo":{"status":"ok","timestamp":1654028114810,"user_tz":180,"elapsed":413,"user":{"displayName":"Alex Souza","userId":"07347463412914112337"}},"outputId":"d9c94ebd-cd3d-41a0-b838-6b584afef0b4"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.6629970129852887, 0.7253160725279748]\n"]}]},{"cell_type":"markdown","source":["### Erro de Propaga√ß√£o Traseira (Back Propagate Error)"],"metadata":{"id":"vLNbj_L_w1as"}},{"cell_type":"markdown","source":["O algoritmo de retropropaga√ß√£o √© nomeado para a maneira como os pesos s√£o treinados.\n","\n","O erro √© calculado entre as sa√≠das esperadas e as sa√≠das propagadas a partir da rede. Esses erros s√£o propagados para tr√°s pela rede, da camada de sa√≠da para a camada oculta, atribuindo a culpa pelo erro e atualizando os pesos √† medida que avan√ßam.\n","\n","A matem√°tica para o erro de retropropaga√ß√£o est√° enraizada no c√°lculo, mas permaneceremos em alto n√≠vel nesta se√ß√£o e focaremos no que √© calculado e como, e n√£o por que, os c√°lculos assumem esse formato espec√≠fico.\n","\n","Esta parte √© dividida em duas se√ß√µes.\n","1. Derivado de transfer√™ncia.\n","2. Backpropagation de erro."],"metadata":{"id":"wvFQyYyyw7DP"}},{"cell_type":"markdown","source":["#### Derivado de transfer√™ncia"],"metadata":{"id":"aic0Gh7xxA8D"}},{"cell_type":"markdown","source":["Dado um valor de sa√≠da de um neur√¥nio, precisamos calcular sua inclina√ß√£o.\n","\n","Estamos usando a fun√ß√£o de transfer√™ncia sigm√≥ide, cuja derivada pode ser calculada da seguinte maneira:\n","`derivative = output * (1.0 - output)`\n","\n","Abaixo est√° uma fun√ß√£o chamada `transfer_derivative()` que implementa esta equa√ß√£o."],"metadata":{"id":"FTsaKOwGxEFh"}},{"cell_type":"code","source":["# Calculate the derivative of an neuron output\n","def transfer_derivative(output):\n","   return output * (1.0 - output)"],"metadata":{"id":"yiVSfZZ1xPFH","executionInfo":{"status":"ok","timestamp":1654031249511,"user_tz":180,"elapsed":285,"user":{"displayName":"Alex Souza","userId":"07347463412914112337"}}},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":["Agora, vamos ver como isso pode ser usado."],"metadata":{"id":"cib_yncoxRf2"}},{"cell_type":"markdown","source":["#### Backpropagation de erro (Error Backpropagation)"],"metadata":{"id":"3I6lqbQxxUC5"}},{"cell_type":"markdown","source":["O primeiro passo √© calcular o erro para cada neur√¥nio de sa√≠da, isso nos dar√° nosso sinal de erro (entrada) para propagar para tr√°s atrav√©s da rede.\n","\n","O erro para um determinado neur√¥nio pode ser calculado da seguinte maneira:\n","`error = (expected - output) * transfer_derivative(output)`\n","\n","Onde `expected` √© o valor de sa√≠da esperado para o neur√¥nio, `output` √© o valor de sa√≠da para o neur√¥nio e `transfer_derivative()` calcula a inclina√ß√£o do valor de sa√≠da do neur√¥nio, como mostrado acima.\n","\n","Este c√°lculo de erro √© usado para neur√¥nios na camada de sa√≠da (output layer). O valor esperado √© o pr√≥prio valor da classe. Na camada oculta, as coisas s√£o um pouco mais complicadas.\n","\n","O sinal de erro de um neur√¥nio na camada oculta √© calculado como o erro ponderado de cada neur√¥nio na camada de sa√≠da. Pense no erro retornando ao longo dos pesos da camada de sa√≠da at√© os neur√¥nios na camada oculta.\n","\n","O sinal de erro propagado de volta √© acumulado e, em seguida, usado para determinar o erro do neur√¥nio na camada oculta, da seguinte maneira:\n","\n","`error = (weight_k * error_j) * transfer_derivative(output)`\n","\n","Onde `error_j` √© o sinal de erro do `j-` √©simo neur√¥nio na camada de sa√≠da, `weight_k` √© o peso que conecta o `k-` √©simo neur√¥nio ao neur√¥nio atual e a sa√≠da √© a sa√≠da do neur√¥nio atual.\n","\n","Abaixo est√° uma fun√ß√£o chamada `backward_propagate_error()` que implementa este procedimento.\n","\n","Voc√™ pode ver que o sinal de erro calculado para cada neur√¥nio √© armazenado com o nome ‚Äòdelta‚Äô. Voc√™ pode ver que as camadas da rede s√£o iteradas na ordem inversa, come√ßando na sa√≠da e trabalhando para tr√°s. Isso garante que os neur√¥nios na camada de sa√≠da tenham valores ‚Äòdelta‚Äô calculados primeiro que os neur√¥nios na camada oculta possam usar na itera√ß√£o subsequente. Eu escolhi o nome ‚Äòdelta‚Äô para refletir a altera√ß√£o que o erro implica no neur√¥nio (por exemplo, o delta do peso).\n","\n","Voc√™ pode ver que o sinal de erro para neur√¥nios na camada oculta √© acumulado a partir de neur√¥nios na camada de sa√≠da, onde o n√∫mero de neur√¥nios ocultos `j` tamb√©m √© o √≠ndice do peso do neur√¥nio no neur√¥nio da camada de sa√≠da `neuron[‚Äòweights‚Äô][j]`."],"metadata":{"id":"b9aRh099xXDE"}},{"cell_type":"code","source":["# Backpropagate error and store in neurons\n","def backward_propagate_error(network, expected):\n","   for i in reversed(range(len(network))):\n","      layer = network[i]\n","      errors = list()\n","      if i != len(network)-1:\n","         for j in range(len(layer)):\n","            error = 0.0\n","            for neuron in network[i + 1]:\n","               error += (neuron['weights'][j] * neuron['delta'])\n","            errors.append(error)\n","      else:\n","         for j in range(len(layer)):\n","            neuron = layer[j]\n","            errors.append(expected[j] - neuron['output'])\n","      for j in range(len(layer)):\n","         neuron = layer[j]\n","         neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])"],"metadata":{"id":"ejiTVx0px_JY","executionInfo":{"status":"ok","timestamp":1654031270922,"user_tz":180,"elapsed":315,"user":{"displayName":"Alex Souza","userId":"07347463412914112337"}}},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":["#### Exemplo - vamos ver funcionando at√© esse trecho"],"metadata":{"id":"dP9peu8WyFQt"}},{"cell_type":"code","source":["# test backpropagation of error\n","network = [[{'output': 0.7105668883115941, 'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}],\n","[{'output': 0.6213859615555266, 'weights': [0.2550690257394217, 0.49543508709194095]}, {'output': 0.6573693455986976, 'weights': [0.4494910647887381, 0.651592972722763]}]]\n","expected = [0, 1]\n","backward_propagate_error(network, expected)\n","for layer in network:\n","   print(layer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7D_Fv9OQyJLW","executionInfo":{"status":"ok","timestamp":1654028547862,"user_tz":180,"elapsed":327,"user":{"displayName":"Alex Souza","userId":"07347463412914112337"}},"outputId":"e079b6e1-1c0a-4637-f6aa-7733f72b2a0d"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["[{'output': 0.7105668883115941, 'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614], 'delta': -0.0005348048046610517}]\n","[{'output': 0.6213859615555266, 'weights': [0.2550690257394217, 0.49543508709194095], 'delta': -0.14619064683582808}, {'output': 0.6573693455986976, 'weights': [0.4494910647887381, 0.651592972722763], 'delta': 0.0771723774346327}]\n"]}]},{"cell_type":"markdown","source":["A execu√ß√£o do exemplo imprime a rede ap√≥s a conclus√£o da backpropagation de erro. Voc√™ pode ver que os valores de erro s√£o calculados e armazenados nos neur√¥nios da camada de sa√≠da e da camada oculta."],"metadata":{"id":"LGKQ9YGIyNnW"}},{"cell_type":"markdown","source":["Agora vamos usar a backpropagation de erro para treinar a rede."],"metadata":{"id":"Uqm_WZlqyQNF"}},{"cell_type":"markdown","source":["### Rede de treinamento (Train Network)"],"metadata":{"id":"IHuVOcd3yR3e"}},{"cell_type":"markdown","source":["A rede √© treinada usando descida de gradiente estoc√°stico.\n","\n","Isso envolve v√°rias itera√ß√µes de exposi√ß√£o de um conjunto de dados de treinamento √† rede e, para cada linha de dados, propagando as entradas, *backpropagating* o erro e atualizando os pesos da rede.\n","\n","Esta parte √© dividida em duas se√ß√µes:\n","1. Atualizar pesos.\n","2. Rede de treinamento."],"metadata":{"id":"1k_d_Hm6yV6I"}},{"cell_type":"markdown","source":["#### Atualizar pesos (Update Weights)"],"metadata":{"id":"BiPTS0AHyc4x"}},{"cell_type":"markdown","source":["Uma vez que os erros s√£o calculados para cada neur√¥nio na rede pelo m√©todo de propaga√ß√£o de retorno acima, eles podem ser usados ‚Äã‚Äãpara atualizar pesos.\n","\n","Os pesos da rede s√£o atualizados da seguinte maneira:\n","\n","`weight = weight + learning_rate * error * input`\n","\n","Onde `weight` √© um dado peso, `learning_rate` √© um par√¢metro que voc√™ deve especificar, `error` √© o erro calculado pelo procedimento de retropropaga√ß√£o para o neur√¥nio e `input` √© o valor de entrada que causou o erro.\n","\n","O mesmo procedimento pode ser usado para atualizar o peso da polariza√ß√£o, exceto que n√£o h√° termo de entrada ou a entrada √© o valor fixo de 1.0.\n","\n","A taxa de aprendizado controla quanto alterar o peso para corrigir o erro. Por exemplo, um valor de 0.1 atualizar√° o peso em 10% da quantia que possivelmente poderia ser atualizada. Preferem-se pequenas taxas de aprendizado que causam aprendizado mais lento em um grande n√∫mero de itera√ß√µes de treinamento. Isso aumenta a probabilidade de a rede encontrar um bom conjunto de pesos em todas as camadas, em vez do conjunto mais r√°pido de pesos que minimiza o erro (chamado converg√™ncia prematura).\n","\n","Abaixo est√° uma fun√ß√£o chamada `update_weights()` que atualiza os pesos para uma rede, dada uma linha de dados de entrada, uma taxa de aprendizado e assume que uma propaga√ß√£o para frente e para tr√°s j√° foi executada.\n","\n","Lembre-se de que a entrada para a camada de sa√≠da √© uma cole√ß√£o de sa√≠das da camada oculta."],"metadata":{"id":"WTkb02uhygRI"}},{"cell_type":"code","source":["# Update network weights with error\n","def update_weights(network, row, l_rate):\n","   for i in range(len(network)):\n","      inputs = row[:-1]\n","      if i != 0:\n","         inputs = [neuron['output'] for neuron in network[i - 1]]\n","      for neuron in network[i]:\n","         for j in range(len(inputs)):\n","            neuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n","         neuron['weights'][-1] += l_rate * neuron['delta']"],"metadata":{"id":"cHcLKQ6Ky39E","executionInfo":{"status":"ok","timestamp":1654031285326,"user_tz":180,"elapsed":288,"user":{"displayName":"Alex Souza","userId":"07347463412914112337"}}},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":["Agora que sabemos como atualizar os pesos da rede, vamos ver como podemos fazer isso repetidamente."],"metadata":{"id":"k962vYujy6q0"}},{"cell_type":"markdown","source":["#### Rede de Treinamento (Train Network)"],"metadata":{"id":"PDKQ_zy_y8jU"}},{"cell_type":"markdown","source":["Como mencionado, a rede √© atualizada usando a descida estoc√°stica do gradiente.\n","\n","Isso envolve o primeiro loop para um n√∫mero fixo de √©pocas e, em cada √©poca, a atualiza√ß√£o da rede para cada linha no conjunto de dados de treinamento.\n","\n","Como s√£o feitas atualiza√ß√µes para cada padr√£o de treinamento, esse tipo de aprendizado √© chamado de aprendizado on-line. Se erros foram acumulados em uma √©poca antes de atualizar os pesos, isso √© chamado aprendizado em lote ou descida em gradiente em lote.\n","\n","Abaixo est√° uma fun√ß√£o que implementa o treinamento de uma rede neural j√° inicializada com um determinado conjunto de dados de treinamento, taxa de aprendizado, n√∫mero fixo de √©pocas e n√∫mero esperado de valores de sa√≠da.\n","\n","O n√∫mero esperado de valores de sa√≠da √© usado para transformar valores de classe nos dados de treinamento em uma codifica√ß√£o quente. Esse √© um vetor bin√°rio com uma coluna para cada valor de classe para corresponder √† sa√≠da da rede. Isso √© necess√°rio para calcular o erro para a camada de sa√≠da.\n","\n","Voc√™ tamb√©m pode ver que o erro da soma ao quadrado entre a sa√≠da esperada e a sa√≠da da rede √© acumulado a cada √©poca e impresso. Isso √© √∫til para criar um rastro do quanto a rede est√° aprendendo e melhorando a cada √©poca."],"metadata":{"id":"azNdmUIRy-6z"}},{"cell_type":"code","source":["# Train a network for a fixed number of epochs\n","def train_network(network, train, l_rate, n_epoch, n_outputs):\n","   for epoch in range(n_epoch):\n","      for row in train:\n","         outputs = forward_propagate(network, row)\n","         expected = [0 for i in range(n_outputs)]\n","         expected[row[-1]] = 1\n","         backward_propagate_error(network, expected)\n","         update_weights(network, row, l_rate)"],"metadata":{"id":"eqbIoQrgy6WF","executionInfo":{"status":"ok","timestamp":1654031296704,"user_tz":180,"elapsed":279,"user":{"displayName":"Alex Souza","userId":"07347463412914112337"}}},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":["Agora temos todas as pe√ßas para treinar a rede. Podemos montar um exemplo que inclui tudo o que vimos at√© agora, incluindo inicializa√ß√£o de rede e treinar uma rede em um pequeno conjunto de dados.\n","\n","Abaixo est√° um pequeno conjunto de dados artificial que podemos usar para testar o treinamento de nossa **rede neural**.\n","\n","```\n","X1          X2           Y\n","2.7810836   2.550537003  0\n","1.465489372 2.362125076  0\n","3.396561688 4.400293529  0\n","1.38807019  1.850220317  0\n","3.06407232  3.005305973  0\n","7.627531214 2.759262235  1\n","5.332441248 2.088626775  1\n","6.922596716 1.77106367   1\n","8.675418651 -0.242068655 1\n","7.673756466 3.508563011  1\n","```\n","\n"],"metadata":{"id":"TdBoTSjszGkn"}},{"cell_type":"markdown","source":["##### Testando\n","\n","Abaixo est√° o exemplo completo. Vamos usar 2 neur√¥nios na camada oculta. √â um problema de classifica√ß√£o bin√°ria (2 classes), portanto haver√° dois neur√¥nios na camada de sa√≠da. A rede ser√° treinada por 20 √©pocas com uma taxa de aprendizado de 0,5, o que √© alto porque estamos treinando para poucas itera√ß√µes."],"metadata":{"id":"dM-Alm0KzT5L"}},{"cell_type":"code","source":["# Test training backprop algorithm\n","seed(1)\n","dataset = [[2.7810836,2.550537003,0],\n","           [1.465489372,2.362125076,0],\n","           [3.396561688,4.400293529,0],\n","           [1.38807019,1.850220317,0],\n","           [3.06407232,3.005305973,0],\n","           [7.627531214,2.759262235,1],\n","           [5.332441248,2.088626775,1],\n","           [6.922596716,1.77106367,1],\n","           [8.675418651,-0.242068655,1],\n","           [7.673756466,3.508563011,1]]\n","\n","n_inputs = len(dataset[0]) - 1\n","n_outputs = len(set([row[-1] for row in dataset]))\n","network = initialize_network(n_inputs, 2, n_outputs)\n","\n","train_network(network, dataset, 0.5, 20, n_outputs)\n","\n","for layer in network:\n","   print(layer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KYRWGg0xzeI8","executionInfo":{"status":"ok","timestamp":1654028976306,"user_tz":180,"elapsed":281,"user":{"displayName":"Alex Souza","userId":"07347463412914112337"}},"outputId":"a7635338-1c29-451e-8ee7-d04170cca21b"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["[{'weights': [-1.4688375095432327, 1.850887325439514, 1.0858178629550297], 'output': 0.029980305604426185, 'delta': -0.0059546604162323625}, {'weights': [0.37711098142462157, -0.0625909894552989, 0.2765123702642716], 'output': 0.9456229000211323, 'delta': 0.0026279652850863837}]\n","[{'weights': [2.515394649397849, -0.3391927502445985, -0.9671565426390275], 'output': 0.23648794202357587, 'delta': -0.04270059278364587}, {'weights': [-2.5584149848484263, 1.0036422106209202, 0.42383086467582715], 'output': 0.7790535202438367, 'delta': 0.03803132596437354}]\n"]}]},{"cell_type":"code","source":["from math import exp\n","from random import seed\n","from random import random\n","\n","# Train a network for a fixed number of epochs\n","def train_network(network, train, l_rate, n_epoch, n_outputs):\n","   for epoch in range(n_epoch):\n","      sum_error = 0\n","   for row in train:\n","      outputs = forward_propagate(network, row)\n","      expected = [0 for i in range(n_outputs)]\n","      expected[row[-1]] = 1\n","      sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n","      backward_propagate_error(network, expected)\n","      update_weights(network, row, l_rate)\n","   print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n","\n","# Test training backprop algorithm\n","seed(1)\n","dataset = [[2.7810836,2.550537003,0],\n","           [1.465489372,2.362125076,0],\n","           [3.396561688,4.400293529,0],\n","           [1.38807019,1.850220317,0],\n","           [3.06407232,3.005305973,0],\n","           [7.627531214,2.759262235,1],\n","           [5.332441248,2.088626775,1],\n","           [6.922596716,1.77106367,1],\n","           [8.675418651,-0.242068655,1],\n","           [7.673756466,3.508563011,1]]\n","n_inputs = len(dataset[0]) - 1\n","n_outputs = len(set([row[-1] for row in dataset]))\n","network = initialize_network(n_inputs, 2, n_outputs)\n","train_network(network, dataset, 0.5, 20, n_outputs)\n","for layer in network:\n","   print(layer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oeCs3KE2z8DY","executionInfo":{"status":"ok","timestamp":1654029069053,"user_tz":180,"elapsed":334,"user":{"displayName":"Alex Souza","userId":"07347463412914112337"}},"outputId":"b81ea871-b3ea-4308-b301-4928ec1b7800"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":[">epoch=19, lrate=0.500, error=6.350\n","[{'weights': [0.04702244528095891, 0.8442898057367274, 0.7548289994028103], 'output': 0.9840590187415943, 'delta': -0.0012432966701953782}, {'weights': [0.1722759810012958, 0.4447229928742587, 0.42635133323053526], 'output': 0.9663709282875607, 'delta': -0.0013702294823488327}]\n","[{'weights': [0.40923052462427756, 0.54380310303148, -0.16580838069853054], 'output': 0.7255320776037567, 'delta': -0.1444790348531614}, {'weights': [-0.11119113899945762, 0.7152027899877591, 0.2916019058174817], 'output': 0.684483563116403, 'delta': 0.06814076441849203}]\n"]}]},{"cell_type":"markdown","source":["A execu√ß√£o do exemplo primeiro imprime o erro da soma ao quadrado a cada √©poca de treinamento. Podemos ver uma tend√™ncia desse erro diminuindo a cada √©poca.\n","\n","Depois de treinada, a rede √© impressa, mostrando os pesos aprendidos. Tamb√©m na rede ainda est√£o os valores de sa√≠da e delta que podem ser ignorados. Poder√≠amos atualizar nossa fun√ß√£o de treinamento para excluir esses dados, se quis√©ssemos."],"metadata":{"id":"lRpDPrixznaV"}},{"cell_type":"markdown","source":["Depois que uma rede √© treinada, precisamos us√°-la para fazer previs√µes. üîÆ"],"metadata":{"id":"SRKvMgiK0P7J"}},{"cell_type":"markdown","source":["### Prever (Predict) üîÆ"],"metadata":{"id":"iTyHkhqe0Xea"}},{"cell_type":"markdown","source":["Fazer previs√µes com uma rede neural treinada √© bastante f√°cil.\n","\n","J√° vimos como propagar adiante um padr√£o de entrada para obter uma sa√≠da. √â tudo o que precisamos fazer para fazer uma previs√£o. Podemos usar os pr√≥prios valores de sa√≠da diretamente como a probabilidade de um padr√£o pertencente a cada classe de sa√≠da.\n","\n","Pode ser mais √∫til transformar essa sa√≠da novamente em uma previs√£o de classe n√≠tida. Podemos fazer isso selecionando o valor da classe com maior probabilidade. Isso tamb√©m √© chamado de [fun√ß√£o arg max](https://en.wikipedia.org/wiki/Arg_max).\n","\n","Abaixo est√° uma fun√ß√£o denominada *predict()* que implementa este procedimento. Retorna o √≠ndice na sa√≠da da rede que tem a maior probabilidade. Parte do princ√≠pio que valores de classe foram convertidos em n√∫meros inteiros come√ßando em 0."],"metadata":{"id":"2I-4uyzH0fuT"}},{"cell_type":"code","source":["# Make a prediction with a network\n","def predict(network, row):\n","   outputs = forward_propagate(network, row)\n","   return outputs.index(max(outputs))"],"metadata":{"id":"pagwXXdE3CbI","executionInfo":{"status":"ok","timestamp":1654031352570,"user_tz":180,"elapsed":308,"user":{"displayName":"Alex Souza","userId":"07347463412914112337"}}},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":["#### Exemplo\n","Podemos juntar isso com o c√≥digo acima para entrada de propaga√ß√£o direta e com nosso pequeno conjunto de dados artificial para testar as previs√µes com uma rede j√° treinada. O exemplo codifica uma rede treinada da etapa anterior.\n","\n","O exemplo completo est√° listado abaixo."],"metadata":{"id":"bv1qe8Ly3GFz"}},{"cell_type":"code","source":["# Test making predictions with the network\n","dataset = [[2.7810836,2.550537003,0],\n","           [1.465489372,2.362125076,0],\n","           [3.396561688,4.400293529,0],\n","           [1.38807019,1.850220317,0],\n","           [3.06407232,3.005305973,0],\n","           [7.627531214,2.759262235,1],\n","           [5.332441248,2.088626775,1],\n","           [6.922596716,1.77106367,1],\n","           [8.675418651,-0.242068655,1],\n","           [7.673756466,3.508563011,1]]\n","network = [[{'weights': [-1.482313569067226, 1.8308790073202204, 1.078381922048799]}, {'weights': [0.23244990332399884, 0.3621998343835864, 0.40289821191094327]}],\n","[{'weights': [2.5001872433501404, 0.7887233511355132, -1.1026649757805829]}, {'weights': [-2.429350576245497, 0.8357651039198697, 1.0699217181280656]}]]\n","\n","for row in dataset:\n","   prediction = predict(network, row)\n","   print('Expected=%d, Got=%d' % (row[-1], prediction))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8U2W8Iyh3LfT","executionInfo":{"status":"ok","timestamp":1654029907559,"user_tz":180,"elapsed":335,"user":{"displayName":"Alex Souza","userId":"07347463412914112337"}},"outputId":"90cb10d1-7e48-423e-a621-77e2c1ebd484"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Expected=0, Got=0\n","Expected=0, Got=0\n","Expected=0, Got=0\n","Expected=0, Got=0\n","Expected=0, Got=0\n","Expected=1, Got=1\n","Expected=1, Got=1\n","Expected=1, Got=1\n","Expected=1, Got=1\n","Expected=1, Got=1\n"]}]},{"cell_type":"markdown","source":["A execu√ß√£o do exemplo imprime a sa√≠da esperada para cada registro no conjunto de dados de treinamento, seguida pela previs√£o precisa feita pela rede.\n","\n","Isso mostra que a rede atinge 100% de acur√°cia (accuracy) nesse pequeno conjunto de dados."],"metadata":{"id":"lxUAf8p55oUE"}},{"cell_type":"markdown","source":["üëΩ Agora estamos prontos para aplicar nosso algoritmo de *backpropagation* a um conjunto de dados do mundo real."],"metadata":{"id":"rJGXD5_N3ZVX"}},{"cell_type":"markdown","source":["### Conjunto de Dados de Sementes de Trigo"],"metadata":{"id":"uHOf2yIq3biJ"}},{"cell_type":"markdown","source":["Esta se√ß√£o aplica o algoritmo *Backpropagation* ao conjunto de dados de sementes de trigo.\n","\n","O primeiro passo √© carregar o conjunto de dados e converter os dados carregados em n√∫meros que podemos usar em nossa rede neural. Para isso, usaremos a fun√ß√£o auxiliar `load_csv()` para carregar o arquivo, `str_column_to_float()` para converter n√∫meros de string em flutuantes e `str_column_to_int()` para converter a coluna da classe em valores inteiros."],"metadata":{"id":"WpqMX6VL3gCt"}},{"cell_type":"code","source":["# Load a CSV file\n","def load_csv(filename):\n","   dataset = list()\n","   with open(filename, 'r') as file:\n","      csv_reader = reader(file)\n","      for row in csv_reader:\n","         if not row:\n","            continue\n","         dataset.append(row)\n","   return dataset"],"metadata":{"id":"j6x9hZP-32OZ","executionInfo":{"status":"ok","timestamp":1654030044918,"user_tz":180,"elapsed":277,"user":{"displayName":"Alex Souza","userId":"07347463412914112337"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["# Convert string column to float\n","def str_column_to_float(dataset, column):\n","   for row in dataset:\n","      row[column] = float(row[column].strip())"],"metadata":{"id":"k2dq2jn5KUZH","executionInfo":{"status":"ok","timestamp":1654030432782,"user_tz":180,"elapsed":3,"user":{"displayName":"Alex Souza","userId":"07347463412914112337"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["# Convert string column to integer\n","def str_column_to_int(dataset, column):\n","   class_values = [row[column] for row in dataset]\n","   unique = set(class_values)\n","   lookup = dict()\n","   for i, value in enumerate(unique):\n","      lookup[value] = i\n","   for row in dataset:\n","      row[column] = lookup[row[column]]\n","   return lookup"],"metadata":{"id":"zKiNBDXpKYHk","executionInfo":{"status":"ok","timestamp":1654030434593,"user_tz":180,"elapsed":5,"user":{"displayName":"Alex Souza","userId":"07347463412914112337"}}},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":["Os valores de entrada variam em escala e precisam ser normalizados para o intervalo de 0 e 1. Geralmente, √© uma boa pr√°tica normalizar valores de entrada para o intervalo da fun√ß√£o de transfer√™ncia escolhida; nesse caso, a fun√ß√£o sigm√≥ide que gera valores entre 0 e 1 As fun√ß√µes auxiliar `dataset_minmax()` e `normalize_dataset()` foram usadas para normalizar os valores de entrada."],"metadata":{"id":"I_mgXz1S4rJW"}},{"cell_type":"code","execution_count":30,"metadata":{"id":"wX-KLBZUJ3Wh","executionInfo":{"status":"ok","timestamp":1654030436689,"user_tz":180,"elapsed":2,"user":{"displayName":"Alex Souza","userId":"07347463412914112337"}}},"outputs":[],"source":["# Find the min and max values for each column\n","def dataset_minmax(dataset):\n","   minmax = list()\n","   stats = [[min(column), max(column)] for column in zip(*dataset)]\n","   return stats"]},{"cell_type":"code","source":["# Rescale dataset columns to the range 0-1\n","def normalize_dataset(dataset, minmax):\n","   for row in dataset:\n","      for i in range(len(row)-1):\n","         row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])"],"metadata":{"id":"cXt93tP9KfSC","executionInfo":{"status":"ok","timestamp":1654030439033,"user_tz":180,"elapsed":2,"user":{"displayName":"Alex Souza","userId":"07347463412914112337"}}},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":["Avaliaremos o algoritmo usando a valida√ß√£o cruzada com dobras k com 5 dobras. Isso significa que 201/5 = 40,2 ou 40 registros estar√£o em cada dobra. Usaremos as fun√ß√µes auxiliar `evaluate_algorithm` | `cross_validation_split` para avaliar o algoritmo com valida√ß√£o cruzada e `accuracy_metric` para calcular a precis√£o das previs√µes."],"metadata":{"id":"m4GZ6VeD4uoM"}},{"cell_type":"code","source":["# Split a dataset into k folds\n","def cross_validation_split(dataset, n_folds):\n","   dataset_split = list()\n","   dataset_copy = list(dataset)\n","   fold_size = int(len(dataset) / n_folds)\n","   for i in range(n_folds):\n","      fold = list()\n","      while len(fold) < fold_size:\n","         index = randrange(len(dataset_copy))\n","         fold.append(dataset_copy.pop(index))\n","      dataset_split.append(fold)\n","   return dataset_split"],"metadata":{"id":"yB0YiTtsKhbp","executionInfo":{"status":"ok","timestamp":1654030440283,"user_tz":180,"elapsed":1,"user":{"displayName":"Alex Souza","userId":"07347463412914112337"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["# Evaluate an algorithm using a cross validation split\n","def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n","   folds = cross_validation_split(dataset, n_folds)\n","   scores = list()\n","   for fold in folds:\n","      train_set = list(folds)\n","      train_set.remove(fold)\n","      train_set = sum(train_set, [])\n","      test_set = list()\n","      for row in fold:\n","         row_copy = list(row)\n","         test_set.append(row_copy)\n","         row_copy[-1] = None\n","      predicted = algorithm(train_set, test_set, *args)\n","      actual    = [row[-1] for row in fold]\n","      accuracy  = accuracy_metric(actual, predicted)\n","      scores.append(accuracy)\n","   return scores"],"metadata":{"id":"dPD64mnAKl5i","executionInfo":{"status":"ok","timestamp":1654030442258,"user_tz":180,"elapsed":3,"user":{"displayName":"Alex Souza","userId":"07347463412914112337"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["# Calculate accuracy percentage\n","def accuracy_metric(actual, predicted):\n","   correct = 0\n","   for i in range(len(actual)):\n","      if actual[i] == predicted[i]:\n","         correct += 1\n","   return correct / float(len(actual)) * 100.0"],"metadata":{"id":"CPaWJUH0KjNw","executionInfo":{"status":"ok","timestamp":1654030444817,"user_tz":180,"elapsed":424,"user":{"displayName":"Alex Souza","userId":"07347463412914112337"}}},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":["Uma nova fun√ß√£o chamada `back_propagation()` foi desenvolvida para gerenciar o aplicativo do algoritmo *Backpropagation*, primeiro inicializando uma rede, treinando-a no conjunto de dados de treinamento e depois usando a rede treinada para fazer previs√µes em um conjunto de dados de teste."],"metadata":{"id":"whxbonSA5FrW"}},{"cell_type":"code","source":["# Backpropagation Algorithm With Stochastic Gradient Descent\n","def back_propagation(train, test, l_rate, n_epoch, n_hidden):\n","   n_inputs = len(train[0]) - 1\n","   n_outputs = len(set([row[-1] for row in train]))\n","   network = initialize_network(n_inputs, n_hidden, n_outputs)\n","   train_network(network, train, l_rate, n_epoch, n_outputs)\n","   predictions = list()\n","   for row in test:\n","      prediction = predict(network, row)\n","      predictions.append(prediction)\n","   return(predictions)"],"metadata":{"id":"org89katLEjT","executionInfo":{"status":"ok","timestamp":1654031427188,"user_tz":180,"elapsed":324,"user":{"displayName":"Alex Souza","userId":"07347463412914112337"}}},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":["\n","O exemplo completo est√° listado abaixo. (as chamadas)"],"metadata":{"id":"zWc4d3c25IFB"}},{"cell_type":"code","source":["# Test Backprop on Seeds dataset\n","seed(1)\n","# load and prepare data\n","filename = 'seeds_dataset.csv'\n","dataset = load_csv(filename)\n","\n","for i in range(len(dataset[0])-1):\n","   str_column_to_float(dataset, i)\n","\n","# convert class column to integers\n","str_column_to_int(dataset, len(dataset[0])-1)\n","\n","# normalize input variables\n","minmax = dataset_minmax(dataset)\n","normalize_dataset(dataset, minmax)\n","\n","# evaluate algorithm\n","n_folds = 5\n","l_rate = 0.3\n","n_epoch = 500\n","n_hidden = 5\n","\n","scores = evaluate_algorithm(dataset, back_propagation, n_folds, l_rate, n_epoch, n_hidden)\n","print('Scores: %s' % scores)\n","print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"90MAJ7RFKdli","executionInfo":{"status":"ok","timestamp":1654031448868,"user_tz":180,"elapsed":16354,"user":{"displayName":"Alex Souza","userId":"07347463412914112337"}},"outputId":"63a001b1-d96e-4156-d375-fbfc67a9fe92"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["Scores: [95.23809523809523, 92.85714285714286, 97.61904761904762, 92.85714285714286, 90.47619047619048]\n","Mean Accuracy: 93.810%\n"]}]},{"cell_type":"markdown","source":["### Exemplo Completo üå±"],"metadata":{"id":"rQN1S2_q9b5t"}},{"cell_type":"code","source":["# Backprop on the Seeds Dataset\n","from random import seed\n","from random import randrange\n","from random import random\n","from csv import reader\n","from math import exp\n","\n","# Load a CSV file\n","def load_csv(filename):\n","   dataset = list()\n","   with open(filename, 'r') as file:\n","      csv_reader = reader(file)\n","      for row in csv_reader:\n","         if not row:\n","            continue\n","         dataset.append(row)\n","   return dataset\n","\n","# Convert string column to float\n","def str_column_to_float(dataset, column):\n","   for row in dataset:\n","      row[column] = float(row[column].strip())\n","\n","# Convert string column to integer\n","def str_column_to_int(dataset, column):\n","   class_values = [row[column] for row in dataset]\n","   unique = set(class_values)\n","   lookup = dict()\n","   for i, value in enumerate(unique):\n","      lookup[value] = i\n","   for row in dataset:\n","      row[column] = lookup[row[column]]\n","   return lookup\n","\n","# Find the min and max values for each column\n","def dataset_minmax(dataset):\n","   minmax = list()\n","   stats = [[min(column), max(column)] for column in zip(*dataset)]\n","   return stats\n","\n","# Rescale dataset columns to the range 0-1\n","def normalize_dataset(dataset, minmax):\n","   for row in dataset:\n","      for i in range(len(row)-1):\n","         row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n","\n","# Split a dataset into k folds\n","def cross_validation_split(dataset, n_folds):\n","   dataset_split = list()\n","   dataset_copy = list(dataset)\n","   fold_size = int(len(dataset) / n_folds)\n","   for i in range(n_folds):\n","      fold = list()\n","      while len(fold) < fold_size:\n","         index = randrange(len(dataset_copy))\n","         fold.append(dataset_copy.pop(index))\n","      dataset_split.append(fold)\n","   return dataset_split\n","\n","# Calculate accuracy percentage\n","def accuracy_metric(actual, predicted):\n","   correct = 0\n","   for i in range(len(actual)):\n","      if actual[i] == predicted[i]:\n","         correct += 1\n","   return correct / float(len(actual)) * 100.0\n","\n","# Evaluate an algorithm using a cross validation split\n","def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n","   folds = cross_validation_split(dataset, n_folds)\n","   scores = list()\n","   for fold in folds:\n","      train_set = list(folds)\n","      train_set.remove(fold)\n","      train_set = sum(train_set, [])\n","      test_set = list()\n","      for row in fold:\n","         row_copy = list(row)\n","         test_set.append(row_copy)\n","         row_copy[-1] = None\n","      predicted = algorithm(train_set, test_set, *args)\n","      actual    = [row[-1] for row in fold]\n","      accuracy  = accuracy_metric(actual, predicted)\n","      scores.append(accuracy)\n","   return scores\n","\n","# Calculate neuron activation for an input\n","def activate(weights, inputs):\n","   activation = weights[-1]\n","   for i in range(len(weights)-1):\n","      activation += weights[i] * inputs[i]\n","   return activation\n","\n","# Transfer neuron activation\n","def transfer(activation):\n","   return 1.0 / (1.0 + exp(-activation))\n","\n","# Forward propagate input to a network output\n","def forward_propagate(network, row):\n","   inputs = row\n","   for layer in network:\n","      new_inputs = []\n","      for neuron in layer:\n","         activation = activate(neuron['weights'], inputs)\n","         neuron['output'] = transfer(activation)\n","         new_inputs.append(neuron['output'])\n","      inputs = new_inputs\n","   return inputs\n","\n","# Calculate the derivative of an neuron output\n","def transfer_derivative(output):\n","   return output * (1.0 - output)\n","\n","# Backpropagate error and store in neurons\n","def backward_propagate_error(network, expected):\n","   for i in reversed(range(len(network))):\n","      layer = network[i]\n","      errors = list()\n","      if i != len(network)-1:\n","         for j in range(len(layer)):\n","            error = 0.0\n","            for neuron in network[i + 1]:\n","               error += (neuron['weights'][j] * neuron['delta'])\n","            errors.append(error)\n","      else:\n","         for j in range(len(layer)):\n","            neuron = layer[j]\n","            errors.append(expected[j] - neuron['output'])\n","      for j in range(len(layer)):\n","         neuron = layer[j]\n","         neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n","\n","# Update network weights with error\n","def update_weights(network, row, l_rate):\n","   for i in range(len(network)):\n","      inputs = row[:-1]\n","      if i != 0:\n","         inputs = [neuron['output'] for neuron in network[i - 1]]\n","      for neuron in network[i]:\n","         for j in range(len(inputs)):\n","            neuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n","         neuron['weights'][-1] += l_rate * neuron['delta']\n","\n","# Train a network for a fixed number of epochs\n","def train_network(network, train, l_rate, n_epoch, n_outputs):\n","   for epoch in range(n_epoch):\n","      for row in train:\n","         outputs = forward_propagate(network, row)\n","         expected = [0 for i in range(n_outputs)]\n","         expected[row[-1]] = 1\n","         backward_propagate_error(network, expected)\n","         update_weights(network, row, l_rate)\n","\n","# Initialize a network\n","def initialize_network(n_inputs, n_hidden, n_outputs):\n","   network = list()\n","   hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n","   network.append(hidden_layer)\n","   output_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n","   network.append(output_layer)\n","   return network\n","\n","# Make a prediction with a network\n","def predict(network, row):\n","   outputs = forward_propagate(network, row)\n","   return outputs.index(max(outputs))\n","\n","# Backpropagation Algorithm With Stochastic Gradient Descent\n","def back_propagation(train, test, l_rate, n_epoch, n_hidden):\n","   n_inputs = len(train[0]) - 1\n","   n_outputs = len(set([row[-1] for row in train]))\n","   network = initialize_network(n_inputs, n_hidden, n_outputs)\n","   train_network(network, train, l_rate, n_epoch, n_outputs)\n","   predictions = list()\n","   for row in test:\n","      prediction = predict(network, row)\n","      predictions.append(prediction)\n","   return(predictions)\n","\n","# Test Backprop on Seeds dataset\n","seed(1)\n","\n","# load and prepare data\n","filename = 'seeds_dataset.csv'\n","dataset = load_csv(filename)\n","for i in range(len(dataset[0])-1):\n","   str_column_to_float(dataset, i)\n","\n","# convert class column to integers\n","str_column_to_int(dataset, len(dataset[0])-1)\n","\n","# normalize input variables\n","minmax = dataset_minmax(dataset)\n","normalize_dataset(dataset, minmax)\n","\n","# evaluate algorithm\n","n_folds = 5\n","l_rate = 0.3\n","n_epoch = 500\n","n_hidden = 5\n","scores = evaluate_algorithm(dataset, back_propagation, n_folds, l_rate, n_epoch, n_hidden)\n","\n","print('Scores: %s' % scores)\n","print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KRcExKVj7Umo","executionInfo":{"status":"ok","timestamp":1654031049173,"user_tz":180,"elapsed":16677,"user":{"displayName":"Alex Souza","userId":"07347463412914112337"}},"outputId":"20c99929-00a4-4e6d-811c-2773cd346ef0"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["Scores: [95.23809523809523, 92.85714285714286, 97.61904761904762, 92.85714285714286, 90.47619047619048]\n","Mean Accuracy: 93.810%\n"]}]},{"cell_type":"markdown","source":["Uma rede com `5 neur√¥nios na camada oculta` e `3 neur√¥nios na camada de sa√≠da` foi constru√≠da. A rede foi treinada por `500 √©pocas` com uma taxa de aprendizado de `0,3`. Esses par√¢metros foram encontrados com um pouco de tentativa e erro, mas voc√™ pode fazer muito melhor.\n","\n","A execu√ß√£o do exemplo imprime a precis√£o m√©dia da classifica√ß√£o em cada dobra, bem como o desempenho m√©dio em todas as dobras.\n","\n","Voc√™ pode ver que a retropropaga√ß√£o e a configura√ß√£o escolhida alcan√ßaram uma precis√£o de classifica√ß√£o m√©dia de cerca de `93%`, o que √© muito melhor que o algoritmo de regra zero, que teve uma precis√£o ligeiramente melhor que `28%` de precis√£o."],"metadata":{"id":"0S2zIfjK5f3h"}},{"cell_type":"markdown","source":["### Sugest√µes de Testes e melhorias"],"metadata":{"id":"qacHLKak_hL5"}},{"cell_type":"markdown","source":["- **Ajustar par√¢metros do algoritmo**. Tente redes maiores ou menores treinadas por mais ou menos. Veja se voc√™ pode obter um melhor desempenho no conjunto de dados de sementes.\n","- **M√©todos adicionais**. Experimente diferentes t√©cnicas de inicializa√ß√£o de peso (como pequenos n√∫meros aleat√≥rios) e diferentes fun√ß√µes de transfer√™ncia (como tanh).\n","- **Mais camadas**. Adicione suporte para mais camadas ocultas, treinadas da mesma maneira que a camada oculta usada neste tutorial.\n","- **Regress√£o**. Altere a rede para que haja apenas um neur√¥nio na camada de sa√≠da e que um valor real seja previsto. Escolha um conjunto de dados de regress√£o para praticar. Uma fun√ß√£o de transfer√™ncia linear pode ser usada para neur√¥nios na camada de sa√≠da ou os valores de sa√≠da do conjunto de dados escolhido podem ser redimensionados para valores entre 0 e 1.\n","- **Descida em gradiente em lote (Batch Gradient Descent)**. Altere o procedimento de treinamento de online para descida em gradiente de lote e atualize os pesos somente no final de cada √©poca."],"metadata":{"id":"Hu80cRms_kcb"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"_2anD57Q_LoC"}},{"cell_type":"markdown","source":["Mais informa√ß√µes sobre Machine Learning? üìö ü§ñ\n","\n","Blog do Zouza no Medium ([link](https://medium.com/blog-do-zouza/o-que-%C3%A9-machine-learning-5e7e98453985))"],"metadata":{"id":"7OQNmXDa_IC7"}}]}